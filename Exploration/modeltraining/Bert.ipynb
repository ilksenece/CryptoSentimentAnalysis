{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d37e360-b295-4586-a568-7004dfc07266",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "3d37e360-b295-4586-a568-7004dfc07266",
     "kernelId": "",
     "source_hidden": false
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\ferhat\\anaconda3\\lib\\site-packages (21.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-22.0.4-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.3.1\n",
      "    Uninstalling pip-21.3.1:\n",
      "      Successfully uninstalled pip-21.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\Ferhat\\\\AppData\\\\Local\\\\Temp\\\\pip-uninstall-eywbcl3k\\\\pip.exe'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81472696",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "81472696",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\ferhat\\anaconda3\\lib\\site-packages (1.20.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\ferhat\\anaconda3\\lib\\site-packages (1.3.4)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\ferhat\\anaconda3\\lib\\site-packages (from pandas) (1.20.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\ferhat\\anaconda3\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\ferhat\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ferhat\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ferhat\\anaconda3\\lib\\site-packages (from sklearn) (0.24.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\ferhat\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.7.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\ferhat\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\ferhat\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.20.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ferhat\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (2.2.0)\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0\n",
      "Requirement already satisfied: torch in c:\\users\\ferhat\\anaconda3\\lib\\site-packages (1.10.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\ferhat\\anaconda3\\lib\\site-packages (from torch) (3.10.0.2)\n",
      "Requirement already satisfied: transformers in c:\\users\\ferhat\\anaconda3\\lib\\site-packages (4.12.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ferhat\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\ferhat\\anaconda3\\lib\\site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ferhat\\anaconda3\\lib\\site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ferhat\\anaconda3\\lib\\site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\ferhat\\anaconda3\\lib\\site-packages (from transformers) (3.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ferhat\\anaconda3\\lib\\site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\ferhat\\anaconda3\\lib\\site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: requests in c:\\users\\ferhat\\anaconda3\\lib\\site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ferhat\\anaconda3\\lib\\site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\ferhat\\anaconda3\\lib\\site-packages (from transformers) (0.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ferhat\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\ferhat\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\ferhat\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ferhat\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ferhat\\anaconda3\\lib\\site-packages (from requests->transformers) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ferhat\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\ferhat\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: click in c:\\users\\ferhat\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (8.0.3)\n",
      "Requirement already satisfied: six in c:\\users\\ferhat\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\ferhat\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install sklearn\n",
    "!pip install torch\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad6461aa",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "ad6461aa",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import torch\n",
    "import transformers\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from transformers import AutoTokenizer, AutoModel \n",
    "from transformers import DataCollatorWithPadding, AutoModelForSequenceClassification\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d39b599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.20.3\n",
      "1.3.4\n",
      "0.24.2\n",
      "1.10.0\n",
      "4.12.5\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "print(skl.__version__)\n",
    "print(torch.__version__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5cf93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleantweets.csv')\n",
    "df = df[['clean_tweet','NumericalLabel']]\n",
    "#batch_1 = df[:10000] \n",
    "df['NumericalLabel'].value_counts()\n",
    "labels = df['NumericalLabel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b7912a-6697-4d11-b6cd-5e0edcb3d7bc",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "96b7912a-6697-4d11-b6cd-5e0edcb3d7bc",
     "kernelId": "",
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.18.5\n",
      "1.1.5\n",
      "0.24.2\n",
      "1.6.0\n",
      "3.5.1\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "print(skl.__version__)\n",
    "print(t.__version__)\n",
    "print(ppb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6d0234",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "7c6d0234",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>NumericalLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Paul Krugman Nobel Luddite I had to tweak th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>But  dum b a ss said you know nothing abou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Another use case for blockchain and Tipper T...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>free coins</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WE are happy to announce that PayVX Presale ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         clean_tweet  NumericalLabel\n",
       "0    Paul Krugman Nobel Luddite I had to tweak th...               1\n",
       "1      But  dum b a ss said you know nothing abou...               1\n",
       "2    Another use case for blockchain and Tipper T...               2\n",
       "3                                        free coins                2\n",
       "4    WE are happy to announce that PayVX Presale ...               2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff03ec6",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "5ff03ec6",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d287cba-dafb-4ef1-87c6-86c0869b0ccb",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "4d287cba-dafb-4ef1-87c6-86c0869b0ccb",
     "kernelId": ""
    }
   },
   "source": [
    "# GPU \n",
    "Declare a variable which will hold the device weâ€™re training on (CPU or GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb32a8f-59c8-495a-8e0d-67ef6c185882",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "ebb32a8f-59c8-495a-8e0d-67ef6c185882",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48a8459-5fce-4e3f-b024-ebdf345a715f",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "e48a8459-5fce-4e3f-b024-ebdf345a715f",
     "kernelId": ""
    }
   },
   "source": [
    "# Preparing the Dataset and Dataloader\n",
    "We will start with defining few key variables that will be used later during the training/fine tuning stage. Followed by creation of Dataset class - This defines how the text is pre-processed before sending it to the neural network. We will also define the Dataloader that will feed the data in batches to the neural network for suitable training and processing. Dataset and Dataloader are constructs of the PyTorch library for defining and controlling the data pre-processing and its passage to neural network. For further reading into Dataset and Dataloader read the docs at PyTorch\n",
    "\n",
    "## Triage Dataset Class\n",
    "This class is defined to accept the Dataframe as input and generate tokenized output that is used by the DistilBERT model for training.\n",
    "We are using the DistilBERT tokenizer to tokenize the data in the TITLE column of the dataframe.\n",
    "The tokenizer uses the encode_plus method to perform tokenization and generate the necessary outputs, namely: ids, attention_mask\n",
    "To read further into the tokenizer, refer to this document\n",
    "target is the encoded category on the news headline.\n",
    "The Triage class is used to create 2 datasets, for training and for validation.\n",
    "Training Dataset is used to fine tune the model: 80% of the original data\n",
    "Validation Dataset is used to evaluate the performance of the model. The model has not seen this data during training.\n",
    "## Dataloader\n",
    "Dataloader is used to for creating training and validation dataloader that load data to the neural network in a defined manner. This is needed because all the data from the dataset cannot be loaded to the memory at once, hence the amount of dataloaded to the memory and then passed to the neural network needs to be controlled.\n",
    "This control is achieved using the parameters such as batch_size and max_len.\n",
    "Training and Validation dataloaders are used in the training and validation part of the flow respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa749b5-8851-405d-a244-32d6cf9292a0",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "1fa749b5-8851-405d-a244-32d6cf9292a0",
     "kernelId": "",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 1e-05\n",
    "#tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8874f5-3287-4f38-aba3-ec78fefc105b",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "4d8874f5-3287-4f38-aba3-ec78fefc105b",
     "kernelId": "",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Triage(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        tweet = str(self.data.clean_tweet[index])\n",
    "        tweet = \" \".join(tweet.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            tweet,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.data.NumericalLabel[index], dtype=torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db74a88-73c4-4873-a779-0d03c8b2faae",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "1db74a88-73c4-4873-a779-0d03c8b2faae",
     "kernelId": "",
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (50859, 2)\n",
      "TRAIN Dataset: (40687, 2)\n",
      "TEST Dataset: (10172, 2)\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset and dataloader for the neural network\n",
    "\n",
    "train_size = 0.8\n",
    "train_dataset=df.sample(frac=train_size,random_state=200)\n",
    "test_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = Triage(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = Triage(test_dataset, tokenizer, MAX_LEN)\n",
    "all_set = Triage(df, tokenizer, MAX_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f4876d-86c0-4988-86ea-a77b253da43b",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "d5f4876d-86c0-4988-86ea-a77b253da43b",
     "kernelId": "",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)\n",
    "full_loader = DataLoader(all_set, **train_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1d8a94-c341-400b-82b7-200261bc85bf",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "aa1d8a94-c341-400b-82b7-200261bc85bf",
     "kernelId": ""
    }
   },
   "source": [
    "# Creating the Neural Network for Fine Tuning\n",
    "## Neural Network\n",
    "We will be creating a neural network with the DistillBERTClass.\n",
    "This network will have the DistilBERT Language model followed by a dropout and finally a Linear layer to obtain the final outputs.\n",
    "The data will be fed to the DistilBERT Language model as defined in the dataset.\n",
    "Final layer outputs is what will be compared to the encoded category to determine the accuracy of models prediction.\n",
    "We will initiate an instance of the network called model. This instance will be used for training and then to save the final trained model for future inference.\n",
    "## Loss Function and Optimizer\n",
    "Loss Function and Optimizer and defined in the next cell.\n",
    "The Loss Function is used the calculate the difference in the output created by the model and the actual output.\n",
    "Optimizer is used to update the weights of the neural network to improve its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db77cf8d-4875-4ba4-b7f5-4e7ff367f442",
   "metadata": {
    "gradient": {
     "editing": true,
     "id": "db77cf8d-4875-4ba4-b7f5-4e7ff367f442",
     "kernelId": "",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
    "\n",
    "class DistillBERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistillBERTClass, self).__init__()\n",
    "        #self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(768, 4)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a61c393-4a92-4b6f-bb98-b04f79a84071",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "3a61c393-4a92-4b6f-bb98-b04f79a84071",
     "kernelId": "",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = DistillBERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb2c9f2-32b3-4058-b251-2e5cfb76cab5",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "dbb2c9f2-32b3-4058-b251-2e5cfb76cab5",
     "kernelId": "",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Creating the loss function and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f5b4e8-70ac-4774-a2b7-cc5d36787a12",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "76f5b4e8-70ac-4774-a2b7-cc5d36787a12",
     "kernelId": ""
    }
   },
   "source": [
    "# Fine Tuning the Model\n",
    "Here we define a training function that trains the model on the training dataset created above, specified number of times (EPOCH), An epoch defines how many times the complete data will be passed through the network.\n",
    "\n",
    "Following events happen in this function to fine tune the neural network:\n",
    "\n",
    "- The dataloader passes data to the model based on the batch size.\n",
    "- Subsequent output from the model and the actual category are compared to calculate the loss.\n",
    "- Loss value is used to optimize the weights of the neurons in the network.\n",
    "- After every 5000 steps the loss value is printed in the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e647a6ef-06c5-4012-b294-dc3a8bfebc78",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "e647a6ef-06c5-4012-b294-dc3a8bfebc78",
     "kernelId": "",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Function to calcuate the accuracy of the model\n",
    "\n",
    "def calcuate_accu(big_idx, targets):\n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ebad23-bde0-4c15-b1d1-f2172e1adeb7",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "e1ebad23-bde0-4c15-b1d1-f2172e1adeb7",
     "kernelId": "",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
    "\n",
    "def train(epoch):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "    for _,data in enumerate(training_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(ids, mask)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        tr_loss += loss.item()\n",
    "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "        n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        if _%5000==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # # When using GPU\n",
    "        optimizer.step()\n",
    "\n",
    "    print('   ')\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e6c522-a58e-4634-ad9a-789f1e3f910b",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "22e6c522-a58e-4634-ad9a-789f1e3f910b",
     "kernelId": "",
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 1.4684494733810425\n",
      "Training Accuracy per 5000 steps: 25.0\n",
      "Training Loss per 5000 steps: 0.2781393148569836\n",
      "Training Accuracy per 5000 steps: 90.7868426314737\n",
      "Training Loss per 5000 steps: 0.19769605163033585\n",
      "Training Accuracy per 5000 steps: 93.73812618738125\n",
      "   \n",
      "The Total Accuracy for Epoch 0: 93.79900213827513\n",
      "Training Loss Epoch: 0.19591861839782815\n",
      "Training Accuracy Epoch: 93.79900213827513\n",
      "Training Loss per 5000 steps: 0.013458801433444023\n",
      "Training Accuracy per 5000 steps: 100.0\n",
      "Training Loss per 5000 steps: 0.054581340301231684\n",
      "Training Accuracy per 5000 steps: 98.49030193961208\n",
      "Training Loss per 5000 steps: 0.05049490184169496\n",
      "Training Accuracy per 5000 steps: 98.64513548645135\n",
      "   \n",
      "The Total Accuracy for Epoch 1: 98.64575908766928\n",
      "Training Loss Epoch: 0.05026594309643077\n",
      "Training Accuracy Epoch: 98.64575908766928\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe602b84-ee4f-49e4-9b00-43517522ee6e",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "fe602b84-ee4f-49e4-9b00-43517522ee6e",
     "kernelId": "",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#def compute_metrics(pred):\n",
    "#    labels = pred.label_ids\n",
    "#    preds = pred.predictions.argmax(-1)\n",
    "#    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "#    acc = accuracy_score(labels, preds)\n",
    "#    return {\n",
    "#        'accuracy': acc,\n",
    "#        'f1': f1,\n",
    "#        'precision': precision,\n",
    "#        'recall': recall\n",
    "#    }\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    #labels = pred.label_ids\n",
    "    #preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    print(f\"Accuracy: {acc}\")\n",
    "    print(f\"f1: {f1}\")\n",
    "    print(f\"precision: {precision}\")\n",
    "    print(f\"recall: {recall}\")\n",
    "    conf_mat = confusion_matrix(labels, preds)\n",
    "    print(f\"Confusion matrix: {conf_mat}\")\n",
    "    cl_report = classification_report(labels, preds)\n",
    "    print(f\"Classification report: {cl_report}\")\n",
    "\n",
    "def valid(model, testing_loader):\n",
    "    model.eval()\n",
    "    list = []\n",
    "    preds = np.array(list)\n",
    "    labels = np.array(list)\n",
    "    n_correct = 0; n_wrong = 0; total = 0; tr_loss = 0; nb_tr_steps= 0; nb_tr_examples=0\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, mask).squeeze()\n",
    "            loss = loss_function(outputs, targets)\n",
    "            tr_loss += loss.item()\n",
    "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "            n_correct += calcuate_accu(big_idx, targets)\n",
    "            preds = np.append(preds,big_idx.to(\"cpu\").numpy())\n",
    "            labels = np.append(labels,targets.to(\"cpu\").numpy())\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples+=targets.size(0)\n",
    "            \n",
    "            if _%5000==0:\n",
    "                loss_step = tr_loss/nb_tr_steps\n",
    "                accu_step = (n_correct*100)/nb_tr_examples\n",
    "                print(f\"Validation Loss per 5000 steps: {loss_step}\")\n",
    "                print(f\"Validation Accuracy per 5000 steps: {accu_step}\")\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "    compute_metrics(preds, labels)\n",
    "    \n",
    "    return epoch_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e837e5-d478-4bdd-a037-e4a1a7e63c3a",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "d7e837e5-d478-4bdd-a037-e4a1a7e63c3a",
     "kernelId": "",
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss per 5000 steps: 0.045048024505376816\n",
      "Validation Accuracy per 5000 steps: 100.0\n",
      "Validation Loss per 5000 steps: 0.06187871896327051\n",
      "Validation Accuracy per 5000 steps: 98.49030193961208\n",
      "Validation Loss Epoch: 0.061835541611588975\n",
      "Validation Accuracy Epoch: 98.48604011010617\n",
      "Accuracy: 0.9848604011010618\n",
      "f1: [0.96181896 0.98767665 0.98800175]\n",
      "precision: [0.95730145 0.98578199 0.99102845]\n",
      "recall: [0.96637931 0.98957861 0.98499348]\n",
      "Confusion matrix: [[1121   22   17]\n",
      " [  22 4368   24]\n",
      " [  28   41 4529]]\n",
      "Classification report:               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.97      0.96      1160\n",
      "         1.0       0.99      0.99      0.99      4414\n",
      "         2.0       0.99      0.98      0.99      4598\n",
      "\n",
      "    accuracy                           0.98     10172\n",
      "   macro avg       0.98      0.98      0.98     10172\n",
      "weighted avg       0.98      0.98      0.98     10172\n",
      "\n",
      "Accuracy on test data = 98.49%\n"
     ]
    }
   ],
   "source": [
    "acc = valid(model, testing_loader)\n",
    "print(\"Accuracy on test data = %0.2f%%\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1f37e8-b5a5-447b-93e3-0894ebdce319",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "6e1f37e8-b5a5-447b-93e3-0894ebdce319",
     "kernelId": "",
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files saved\n"
     ]
    }
   ],
   "source": [
    "# Saving the files for re-use\n",
    "\n",
    "#output_model_file = './SavedModels/pytorch_distilbert_news.bin'\n",
    "#output_vocab_file = './SavedModels/vocab_distilbert_news.bin'\n",
    "\n",
    "#model_to_save = model\n",
    "#torch.save(model_to_save, output_model_file)\n",
    "#tokenizer.save_vocabulary(output_vocab_file)\n",
    "\n",
    "model_dir = './SavedModels/'\n",
    "\n",
    "# Specify a path\n",
    "PATH = model_dir + \"pytorch_distilbert_sent_uncased.pt\"\n",
    "\n",
    "# Save\n",
    "torch.save(model, PATH)\n",
    "\n",
    "#model.save_pretrained(model_dir+'pytorch_distilbert_sent_uncased/model')\n",
    "tokenizer.save_pretrained(model_dir)\n",
    "\n",
    "print('All files saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb68aeb-62d7-4ac1-9458-eb59d31bcfb1",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "0cb68aeb-62d7-4ac1-9458-eb59d31bcfb1",
     "kernelId": "",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model_dir = './SavedModels/'\n",
    "PATH = model_dir + \"pytorch_distilbert_sent_uncased.pt\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4242a7-70a2-4e2a-b221-94e0507b36d9",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "4c4242a7-70a2-4e2a-b221-94e0507b36d9",
     "kernelId": "",
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistillBERTClass(\n",
       "  (l1): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = torch.load(\"./SavedModels/pytorch_distilbert_news.bin\")\n",
    "#tokenizer = DistilBertTokenizer.from_pretrained(\"./SavedModels/\")\n",
    "\n",
    "# retreive the saved model \n",
    "model_dir = './SavedModels/'\n",
    "PATH = model_dir + \"pytorch_distilbert_sent_uncased.pt\"\n",
    "#model = DistilBertForTokenClassification.from_pretrained(model_dir + 'pytorch_distilbert_sent_uncased/model', \n",
    "#                                                        local_files_only=True)\n",
    "model = DistillBERTClass()\n",
    "model = torch.load(PATH)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ee2681-c18d-4158-93f1-9a5e5af7b8dc",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "12ee2681-c18d-4158-93f1-9a5e5af7b8dc",
     "kernelId": "",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def Predict(new_tweet, label, tokenizer, model):\n",
    "    data = [[new_tweet, label]]\n",
    " \n",
    "    # Create the pandas DataFrame\n",
    "    df = pd.DataFrame(data, columns = ['clean_tweet', 'NumericalLabel']) \n",
    "    df_set = Triage(df, tokenizer, MAX_LEN)\n",
    "\n",
    "    test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "    df_loader = DataLoader(df_set, **test_params)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _,data in enumerate(df_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, mask)\n",
    "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "    return big_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a0b1f4-d237-46f4-b0eb-27a938134853",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "68a0b1f4-d237-46f4-b0eb-27a938134853",
     "kernelId": "",
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1], device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tweet = \"I am neutral to cryptocurrencies\"\n",
    "outputs = Predict(new_tweet, 1, tokenizer, model)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea4e3be",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "1ea4e3be",
     "kernelId": ""
    }
   },
   "source": [
    "# Evaluating Model #2\n",
    "So how well does our model do in classifying sentences? One way is to check the accuracy against the testing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ded6cc8",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "3ded6cc8",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[201,  47,  51],\n",
       "       [ 33, 848, 144],\n",
       "       [ 23, 165, 988]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_labels, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f83e05",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "79f83e05",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       0.78      0.67      0.72       299\\n           1       0.80      0.83      0.81      1025\\n           2       0.84      0.84      0.84      1176\\n\\n    accuracy                           0.81      2500\\n   macro avg       0.81      0.78      0.79      2500\\nweighted avg       0.81      0.81      0.81      2500\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(test_labels, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e4a95f",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "29e4a95f",
     "kernelId": ""
    }
   },
   "source": [
    "How good is this score? What can we compare it against? Let's first look at a dummy classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31260d33",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "31260d33",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy classifier score: 0.460 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "clf = DummyClassifier()\n",
    "\n",
    "scores = cross_val_score(clf, train_features, train_labels)\n",
    "print(\"Dummy classifier score: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163bfccd",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "163bfccd",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
